{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2abebe4-7c06-47a3-adc6-eb21ca9601bd",
   "metadata": {},
   "source": [
    "# Extracting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7c5ad1-e0b1-4e07-a60b-6348a1d76ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import filetype, io, pytesseract, pdfplumber, docx\n",
    "from PIL import Image\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def extract_text(raw_text):   \n",
    "   if isinstance(raw_text, memoryview):\n",
    "    raw_text = raw_text.tobytes()  # Convert memoryview to bytes\n",
    "       \n",
    "   file_extension = filetype.guess_extension(raw_text)\n",
    "   \n",
    "   if file_extension == 'pdf':\n",
    "         with pdfplumber.open(io.BytesIO(raw_text)) as pdf:\n",
    "            resume_text = []\n",
    "            for page in pdf.pages:\n",
    "               resume_text.append(page.extract_text() or \"\")\n",
    "            resume_text = ' '.join(resume_text)\n",
    "            \n",
    "      \n",
    "   elif file_extension == 'docx':\n",
    "      document = docx.Document(io.BytesIO(raw_text))\n",
    "      resume_text = ' '.join([para.text for para in document.paragraphs])\n",
    "\n",
    "   elif file_extension in ['png', 'jpg', 'jpeg']:\n",
    "       image = Image.open(io.BytesIO(raw_text))\n",
    "       resume_text = pytesseract.image_to_string(image)\n",
    "       \n",
    "   else:\n",
    "      resume_text = raw_text.decode('utf-8', errors='ignore')\n",
    "   \n",
    "   return resume_text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ceef57-0939-4c67-896e-ef61f646eb96",
   "metadata": {},
   "source": [
    "# Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03ec484-9a83-444d-bcf3-c82e4d9a02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):  \n",
    " text = re.sub(r'[\\u2022\\u2023\\u25E6\\u2043\\u2219]', '-', text) #removing common bullets\n",
    " text = re.sub(r'[\\s+]', ' ', text) #removing any whitespace character\n",
    " text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) #match anything\n",
    " return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69475c-bc71-4db5-ba46-a4ab593bfd6d",
   "metadata": {},
   "source": [
    "# Sentence Splitting and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15185fdf-1e8b-4a0a-bf80-f18b5443bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os, spacy, numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "model_path = 'models/mpnet_local'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "   model = SentenceTransformer(model_path, device='cpu')\n",
    "\n",
    "else:\n",
    "  model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "  model.save(model_path)\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "def sentence_embedding(text):\n",
    "  doc = nlp(text)\n",
    "  sentence_list = [sent.text.strip() for sent in doc.sents]\n",
    "  sentence_vector = model.encode(\n",
    "                      sentence_list, \n",
    "                      output_value='sentence_embedding',\n",
    "                      convert_to_numpy=True,\n",
    "                      convert_to_tensor=False,\n",
    "                      batch_size=24,\n",
    "                      normalize_embeddings=True,\n",
    "                      device='cpu'\n",
    "                    )\n",
    "  return sentence_vector\n",
    "\n",
    "def normalized_mean_of_sentence_vector(sentence_vector):\n",
    "    mean_vector = np.mean(np.array(sentence_vector), axis=0)\n",
    "    #print(mean_vector.shape)\n",
    "    norm = np.linalg.norm(mean_vector)\n",
    "    if norm == 0 or np.isnan(norm):\n",
    "        return None\n",
    "    normalized_vector = mean_vector / norm\n",
    "    #print(np.linalg.norm(normalized_vector))\n",
    "    return normalized_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc73b0-eedf-4b07-b943-6af7e5e08ffc",
   "metadata": {},
   "source": [
    "# Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638adb8a-9207-41e9-b9c8-aadb986fbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(raw_text):\n",
    "    extracted_text = extract_text(raw_text) # extracting text\n",
    "    if not extracted_text or not extracted_text.strip():\n",
    "        return None, None\n",
    "        \n",
    "    cleaned_text = clean_text(extracted_text) # cleaning text\n",
    "    if not cleaned_text or not cleaned_text.strip():\n",
    "        return None, None\n",
    "        \n",
    "    sentence_vector = sentence_embedding(cleaned_text) # embedding sentence\n",
    "    if sentence_vector is None or len(sentence_vector)==0:\n",
    "        return None, None\n",
    "        \n",
    "    normalized_mean_vector = normalized_mean_of_sentence_vector(sentence_vector)\n",
    "    return normalized_mean_vector, cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d18301-6137-476f-bc8b-7576a998b5d0",
   "metadata": {},
   "source": [
    "# Just finding out source of resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285930c9-6005-4a27-bfab-372a56259657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCOUNTANT',\n",
       " 'ADVOCATE',\n",
       " 'AGRICULTURE',\n",
       " 'APPAREL',\n",
       " 'ARTS',\n",
       " 'AUTOMOBILE',\n",
       " 'AVIATION',\n",
       " 'BANKING',\n",
       " 'BPO',\n",
       " 'BUSINESS-DEVELOPMENT',\n",
       " 'CHEF',\n",
       " 'CONSTRUCTION',\n",
       " 'CONSULTANT',\n",
       " 'DESIGNER',\n",
       " 'DIGITAL-MEDIA',\n",
       " 'ENGINEERING',\n",
       " 'FINANCE',\n",
       " 'FITNESS',\n",
       " 'HEALTHCARE',\n",
       " 'HR',\n",
       " 'INFORMATION-TECHNOLOGY',\n",
       " 'PUBLIC-RELATIONS',\n",
       " 'SALES',\n",
       " 'TEACHER']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, hashlib\n",
    "base_path = 'CVS'\n",
    "clean_path = 'Clean_CV'\n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "\n",
    "hash_file = os.path.join(clean_path, 'file_hashes.txt')\n",
    "\n",
    "if os.path.exists(hash_file):\n",
    "    with open(hash_file, 'r') as f:\n",
    "        existing_hashes = set(line.strip() for line in f)\n",
    "\n",
    "else:\n",
    "    existing_hashes = set()\n",
    "\n",
    "folder_source = []\n",
    "for folder in os.listdir(base_path):\n",
    "    path = os.path.join(base_path, folder)\n",
    "    if os.path.isdir(path):\n",
    "        folder_source.append(folder)\n",
    "folder_source = sorted(folder_source)\n",
    "folder_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1ddf3-3c01-4d28-9ed2-f61e8bb20a3d",
   "metadata": {},
   "source": [
    "# Storing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b223906e-45b9-4668-8889-db15a9eb9bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os, hashlib\n",
    "\n",
    "# base_path = 'CVS'\n",
    "# clean_path = 'Clean_CV'\n",
    "\n",
    "# os.makedirs(base_path, exist_ok=True)\n",
    "# os.makedirs(clean_path, exist_ok=True)\n",
    "\n",
    "# hash_path = os.path.join(clean_path, 'file_hashes.txt')\n",
    "\n",
    "# existing_hashes = set()\n",
    "# if os.path.exists(hash_path):\n",
    "#     with open(hash_path, 'r') as f:\n",
    "#         for line in f:\n",
    "#             existing_hashes.add(line.strip())\n",
    "\n",
    "# for folder in folder_source:\n",
    "#     path = os.path.join(base_path, folder)\n",
    "#     save_path = os.path.join(clean_path, folder)\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#     current_folder_vectors = []\n",
    "#     current_folder_hashes = []\n",
    "    \n",
    "#     for file in sorted(os.listdir(path)): \n",
    "#         file_path = os.path.join(path, file)\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             text = f.read()\n",
    "        \n",
    "#         resume_vector, resume_text = pipeline(text)\n",
    "#         resume_vector = np.array(resume_vector)\n",
    "#         if resume_vector is None or resume_vector.size != 768:\n",
    "#             actual_size = resume_vector.size if hasattr(resume_vector, 'size') else 'Unknown'\n",
    "#             print(f\"Skipped invalid resume (wrong shape or empty) : {file}, size: {actual_size}\")\n",
    "#             continue\n",
    "\n",
    "#         file_hash = hashlib.sha256(resume_text.encode('utf-8')).hexdigest()\n",
    "#         if file_hash in existing_hashes:\n",
    "#             print(f'Skipped duplicate: {file}')\n",
    "#             continue\n",
    "            \n",
    "#         existing_hashes.add(file_hash)   \n",
    "#         current_folder_hashes.append(file_hash)\n",
    "#         current_folder_vectors.append(resume_vector)\n",
    "#         print(len(current_folder_vectors))\n",
    "\n",
    "#     if current_folder_hashes:\n",
    "#         with open(hash_path, 'a') as f:\n",
    "#             f.write('\\n'.join(current_folder_hashes) + '\\n')\n",
    "#             f.write(f\"{folder}\\n\")\n",
    "    \n",
    "#     if current_folder_vectors:\n",
    "#         np_vector = np.vstack(current_folder_vectors)\n",
    "#         mean_vector = np.mean(np_vector, axis=0)\n",
    "#         norm = np.linalg.norm(mean_vector)\n",
    "#         if norm == 0 or np.isnan(norm):\n",
    "#             print('Invalid mean vector')\n",
    "#             continue\n",
    "#         mean_vector = mean_vector / norm\n",
    "#         final_vector_path = os.path.join(save_path, 'mean_vector.npz')\n",
    "#         np.savez(final_vector_path, mean=mean_vector, all_vectors=np_vector)\n",
    "#         print(f'Saved both mean and all vectors for {folder} to {final_vector_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38120f-373e-4e01-b247-75f29a3dfeb7",
   "metadata": {},
   "source": [
    "# Collecting mean embedding of each domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85d60ed-d832-4c49-af94-e5ac022598a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "domain_dict ={}\n",
    "all_vectors, all_vectors_labels = [], []\n",
    "clean_path = 'Clean_CV'\n",
    "folder_source = []\n",
    "for folder in os.listdir(clean_path):\n",
    "    path = os.path.join(clean_path, folder)\n",
    "    if os.path.isdir(path):\n",
    "        folder_source.append(folder)\n",
    "\n",
    "folder_source = sorted(folder_source)\n",
    "for folder in folder_source:\n",
    "    path = os.path.join(clean_path, folder)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.npz'):\n",
    "            with np.load(os.path.join(path, file)) as data:\n",
    "                vector = data['all_vectors']\n",
    "                all_vectors.extend(vector)\n",
    "                all_vectors_labels.extend([folder]*vector.shape[0])\n",
    "                domain_dict[folder] = data['mean']\n",
    "\n",
    "\n",
    "final_dict = {\n",
    "    'domain_value': domain_dict,\n",
    "    'each_vector': np.vstack(all_vectors),\n",
    "    'each_label': np.array(all_vectors_labels)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d418b3e-f0a5-478e-98f7-91547ea890c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('final.pkl', 'wb') as f:\n",
    "    pickle.dump(final_dict, f)\n",
    "print('File created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5c0b0-2c56-443c-a3ad-6fff09cbf1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6c30954-a799-449b-9c30-f85b1983c5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4be1a0cfb4c4c7fadd68407d3161cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf, .docx, .png, .txt, .jpg, .jpeg', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.pdf, .docx, .png, .txt, .jpg, .jpeg'\n",
    ")\n",
    "\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4697748-30d5-4e2c-a3aa-37089c5349f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file uploaded\n"
     ]
    }
   ],
   "source": [
    "if uploader.value:\n",
    "    file = uploader.value[0]\n",
    "    print('file uploaded')\n",
    "else:\n",
    "    print('file not uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7437be27-051a-4c30-be5f-d912d6272030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MIT-Based Cryptography Roadmap Introduction & Prerequisites This roadmap outlines a structured path to learn cryptography following MIT-level standards. Basic algebra, discrete math, and Python are recommended prerequisites. Phase 1: Foundations (Math   Intro Crypto) (cid:127) Discrete mathematics (cid:127) Number theory (cid:127) Modular arithmetic (cid:127) Basic cryptographic primitives (cid:127) Hashing, symmetric encryption, one-way functions Phase 2: Applied Cryptography (cid:127) AES, RSA, ECC (cid:127) TLS, HTTPS, PKI (cid:127) Digital signatures (cid:127) Secure coding practices (cid:127) Realnworld protocol failures Phase 3: Advanced Theory (cid:127) Zeronknowledge proofs (cid:127) Secure multiparty computation (cid:127) Lattices & postnquantum crypto (cid:127) Complexity theory foundations Phase 4: Research Topics (cid:127) Homomorphic encryption (cid:127) Cryptanalysis (cid:127) Proof-carrying data (cid:127) Cuttingnedge postnquantum systems Recommended Textbooks (cid:127) Introduction to Modern Cryptography   Katz & Lindell (cid:127) Understanding Cryptography   Paar & Pelzl (cid:127) Mathematics of Cryptography   MIT OCW notes (cid:127) Applied Cryptography   Schneier 12nMonth Study Schedule (cid:127) Months 1 3: Math   Intro Cryptography (cid:127) Months 4 6: Applied Cryptography (cid:127) Months 7 9: Advanced Theory (cid:127) Months 10 12: Research topics   projects YouTube / OCW Links (cid:127) MIT 6.875 Cryptography (cid:127) MIT 6.042 Mathematics for Computer Science (cid:127) Computerphile Cryptography playlist (cid:127) Stanford applied crypto lectures Checklists & Progress Tracker (cid:127) Phase completion checklist (cid:127) Topic mastery checklist (cid:127) Monthly progress log (cid:127) Goal tracking templates included'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_resume_vector, new_resume_text = pipeline(file['content'])\n",
    "new_resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d536fe57-7655-4d20-a930-14d3409fd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_resume_vector is None:\n",
    "    print('Something is wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a243569d-cc5a-43b7-b763-6f48421b75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Use 'rb' (Read Binary)\n",
    "with open('final.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be1a7224-ccb6-414d-b87c-9d525a61a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.21513158]], dtype=float32), array([[0.18158045]], dtype=float32), array([[0.22961742]], dtype=float32), array([[0.1494559]], dtype=float32), array([[0.23032983]], dtype=float32), array([[0.20712756]], dtype=float32), array([[0.20988353]], dtype=float32), array([[0.21220729]], dtype=float32), array([[0.22544411]], dtype=float32), array([[0.15642521]], dtype=float32), array([[0.11708594]], dtype=float32), array([[0.18746476]], dtype=float32), array([[0.2371529]], dtype=float32), array([[0.21156885]], dtype=float32), array([[0.19402468]], dtype=float32), array([[0.2592247]], dtype=float32), array([[0.20530562]], dtype=float32), array([[0.16421136]], dtype=float32), array([[0.17343241]], dtype=float32), array([[0.1751014]], dtype=float32), array([[0.29557395]], dtype=float32), array([[0.17048492]], dtype=float32), array([[0.13869867]], dtype=float32), array([[0.22874132]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim = {'labels': [], 'cosine_sim': []}\n",
    "for label, vector in data['domain_value'].items():\n",
    "    sim['labels'].append(label)\n",
    "    sim['cosine_sim'].append(cosine_similarity(new_resume_vector.reshape(1,-1), vector.reshape(1,-1)))\n",
    "print(sim['cosine_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6055b904-fcb7-4d37-9783-9012273f6f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29557395]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_vector = max(sim['cosine_sim'])\n",
    "# i=0\n",
    "# for vec in sim['cosine_sim']:\n",
    "#     if vec == max_vector:\n",
    "#         break\n",
    "#     i+=1\n",
    "# print(i)\n",
    "# label = sim['labels'][i]\n",
    "# label\n",
    "\n",
    "max_index = sim['cosine_sim'].index(max(sim['cosine_sim']))\n",
    "match_label = sim['labels'][max_index]\n",
    "print(max(sim['cosine_sim']))\n",
    "match_label\n",
    "a = [2,4,5,3,5,1]\n",
    "max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ad32289-e31f-46cc-a8d6-dada1f3723ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match: INFORMATION-TECHNOLOGY with similarity [[0.29557395]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCOUNTANT</td>\n",
       "      <td>[[0.21513158]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADVOCATE</td>\n",
       "      <td>[[0.18158045]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGRICULTURE</td>\n",
       "      <td>[[0.22961742]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APPAREL</td>\n",
       "      <td>[[0.1494559]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTS</td>\n",
       "      <td>[[0.23032983]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>[[0.20712756]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AVIATION</td>\n",
       "      <td>[[0.20988353]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BANKING</td>\n",
       "      <td>[[0.21220729]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BPO</td>\n",
       "      <td>[[0.22544411]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BUSINESS-DEVELOPMENT</td>\n",
       "      <td>[[0.15642521]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHEF</td>\n",
       "      <td>[[0.11708594]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CONSTRUCTION</td>\n",
       "      <td>[[0.18746476]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CONSULTANT</td>\n",
       "      <td>[[0.2371529]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DESIGNER</td>\n",
       "      <td>[[0.21156885]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DIGITAL-MEDIA</td>\n",
       "      <td>[[0.19402468]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ENGINEERING</td>\n",
       "      <td>[[0.2592247]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FINANCE</td>\n",
       "      <td>[[0.20530562]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FITNESS</td>\n",
       "      <td>[[0.16421136]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HEALTHCARE</td>\n",
       "      <td>[[0.17343241]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HR</td>\n",
       "      <td>[[0.1751014]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>INFORMATION-TECHNOLOGY</td>\n",
       "      <td>[[0.29557395]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PUBLIC-RELATIONS</td>\n",
       "      <td>[[0.17048492]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SALES</td>\n",
       "      <td>[[0.13869867]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TEACHER</td>\n",
       "      <td>[[0.22874132]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    labels      cosine_sim\n",
       "0               ACCOUNTANT  [[0.21513158]]\n",
       "1                 ADVOCATE  [[0.18158045]]\n",
       "2              AGRICULTURE  [[0.22961742]]\n",
       "3                  APPAREL   [[0.1494559]]\n",
       "4                     ARTS  [[0.23032983]]\n",
       "5               AUTOMOBILE  [[0.20712756]]\n",
       "6                 AVIATION  [[0.20988353]]\n",
       "7                  BANKING  [[0.21220729]]\n",
       "8                      BPO  [[0.22544411]]\n",
       "9     BUSINESS-DEVELOPMENT  [[0.15642521]]\n",
       "10                    CHEF  [[0.11708594]]\n",
       "11            CONSTRUCTION  [[0.18746476]]\n",
       "12              CONSULTANT   [[0.2371529]]\n",
       "13                DESIGNER  [[0.21156885]]\n",
       "14           DIGITAL-MEDIA  [[0.19402468]]\n",
       "15             ENGINEERING   [[0.2592247]]\n",
       "16                 FINANCE  [[0.20530562]]\n",
       "17                 FITNESS  [[0.16421136]]\n",
       "18              HEALTHCARE  [[0.17343241]]\n",
       "19                      HR   [[0.1751014]]\n",
       "20  INFORMATION-TECHNOLOGY  [[0.29557395]]\n",
       "21        PUBLIC-RELATIONS  [[0.17048492]]\n",
       "22                   SALES  [[0.13869867]]\n",
       "23                 TEACHER  [[0.22874132]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sim)\n",
    "max_row = df.loc[df['cosine_sim'].idxmax()]\n",
    "print(f'Best match: {max_row['labels']} with similarity {max_row['cosine_sim']}')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2984e-639d-4c21-afb2-38e9dbf36474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd12743-4cec-4ba9-be6d-eddf4ec0345d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
