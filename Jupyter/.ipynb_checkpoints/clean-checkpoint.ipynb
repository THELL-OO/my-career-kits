{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2abebe4-7c06-47a3-adc6-eb21ca9601bd",
   "metadata": {},
   "source": [
    "# Extracting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7c5ad1-e0b1-4e07-a60b-6348a1d76ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import filetype, io, pytesseract, pdfplumber, docx\n",
    "from PIL import Image\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def extract_text(raw_text):   \n",
    "   if isinstance(raw_text, memoryview):\n",
    "    raw_text = raw_text.tobytes()  # Convert memoryview to bytes\n",
    "       \n",
    "   file_extension = filetype.guess_extension(raw_text)\n",
    "   \n",
    "   if file_extension == 'pdf':\n",
    "         with pdfplumber.open(io.BytesIO(raw_text)) as pdf:\n",
    "            resume_text = []\n",
    "            for page in pdf.pages:\n",
    "               resume_text.append(page.extract_text() or \"\")\n",
    "            resume_text = ' '.join(resume_text)\n",
    "            \n",
    "      \n",
    "   elif file_extension == 'docx':\n",
    "      document = docx.Document(io.BytesIO(raw_text))\n",
    "      resume_text = ' '.join([para.text for para in document.paragraphs])\n",
    "\n",
    "   elif file_extension in ['png', 'jpg', 'jpeg']:\n",
    "       image = Image.open(io.BytesIO(raw_text))\n",
    "       resume_text = pytesseract.image_to_string(image)\n",
    "       \n",
    "   else:\n",
    "      resume_text = raw_text.decode('utf-8', errors='ignore')\n",
    "   \n",
    "   return resume_text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ceef57-0939-4c67-896e-ef61f646eb96",
   "metadata": {},
   "source": [
    "# Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ec484-9a83-444d-bcf3-c82e4d9a02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):  \n",
    " text = re.sub(r'[\\u2022\\u2023\\u25E6\\u2043\\u2219]', '-', text) #removing common bullets\n",
    " text = re.sub(r'[\\s+]', ' ', text) #removing any whitespace character\n",
    " text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) #match anything\n",
    " return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69475c-bc71-4db5-ba46-a4ab593bfd6d",
   "metadata": {},
   "source": [
    "# Sentence Splitting and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15185fdf-1e8b-4a0a-bf80-f18b5443bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os, spacy, numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "model_path = 'models/mpnet_local'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "   model = SentenceTransformer(model_path, device='cpu')\n",
    "\n",
    "else:\n",
    "  model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "  model.save(model_path)\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "def sentence_embedding(text):\n",
    "  doc = nlp(text)\n",
    "  sentence_list = [sent.text.strip() for sent in doc.sents]\n",
    "  sentence_vector = model.encode(\n",
    "                      sentence_list, \n",
    "                      output_value='sentence_embedding',\n",
    "                      convert_to_numpy=True,\n",
    "                      convert_to_tensor=False,\n",
    "                      batch_size=24,\n",
    "                      normalize_embeddings=True,\n",
    "                      device='cpu'\n",
    "                    )\n",
    "  return sentence_vector\n",
    "\n",
    "def normalized_mean_of_sentence_vector(sentence_vector):\n",
    "    mean_vector = np.mean(np.array(sentence_vector), axis=0)\n",
    "    #print(mean_vector.shape)\n",
    "    normalized_vector = mean_vector / np.linalg.norm(mean_vector)\n",
    "    #print(np.linalg.norm(normalized_vector))\n",
    "    return normalized_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc73b0-eedf-4b07-b943-6af7e5e08ffc",
   "metadata": {},
   "source": [
    "# Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638adb8a-9207-41e9-b9c8-aadb986fbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(raw_text):\n",
    "    extracted_text = extract_text(raw_text) # extracting text\n",
    "    cleaned_text = clean_text(extracted_text) # cleaning text\n",
    "    sentence_vector = sentence_embedding(cleaned_text) # embedding sentence\n",
    "    normalized_mean_vector = normalized_mean_of_sentence_vector(sentence_vector)\n",
    "    return normalized_mean_vector, cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d18301-6137-476f-bc8b-7576a998b5d0",
   "metadata": {},
   "source": [
    "# Just finding out source of resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285930c9-6005-4a27-bfab-372a56259657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, hashlib\n",
    "base_path = 'CVS'\n",
    "clean_path = 'Clean_CV'\n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "\n",
    "hash_file = os.path.join(clean_path, 'file_hashes.txt')\n",
    "\n",
    "if os.path.exists(hash_file):\n",
    "    with open(hash_file, 'r') as f:\n",
    "        existing_hashes = set(line.strip() for line in f)\n",
    "\n",
    "else:\n",
    "    existing_hashes = set()\n",
    "\n",
    "folder_source = []\n",
    "for folder in os.listdir(base_path):\n",
    "    if folder == 'file_hashes.txt':\n",
    "        continue\n",
    "    folder_source.append(folder)\n",
    "folder_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1ddf3-3c01-4d28-9ed2-f61e8bb20a3d",
   "metadata": {},
   "source": [
    "# Storing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223906e-45b9-4668-8889-db15a9eb9bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, hashlib\n",
    "\n",
    "base_path = 'CVS'\n",
    "clean_path = 'Clean_CV'\n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "os.makedirs(clean_path, exist_ok=True)\n",
    "\n",
    "hash_path = os.path.join(clean_path, 'file_hashes.txt')\n",
    "\n",
    "existing_hashes = set()\n",
    "if os.path.exists(hash_path):\n",
    "    with open(hash_path, 'r') as f:\n",
    "        for line in f:\n",
    "            existing_hashes.add(line.strip())\n",
    "\n",
    "for folder in folder_source:\n",
    "    path = os.path.join(base_path, folder)\n",
    "    save_path = os.path.join(clean_path, folder)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    current_folder_vectors = []\n",
    "    current_folder_hashes = []\n",
    "    \n",
    "    for file in sorted(os.listdir(path)): \n",
    "        file_path = os.path.join(path, file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        resume_vector, resume_text = pipeline(text)\n",
    "\n",
    "        file_hash = hashlib.sha256(resume_text.encode('utf-8')).hexdigest()\n",
    "        if file_hash in existing_hashes:\n",
    "            print(f'Skipped duplicate: {file}')\n",
    "            continue\n",
    "            \n",
    "        existing_hashes.add(file_hash)   \n",
    "        current_folder_hashes.append(file_hash)\n",
    "        current_folder_vectors.append(resume_vector)\n",
    "\n",
    "    if current_folder_hashes:\n",
    "        with open(hash_path, 'a') as f:\n",
    "            f.write('\\n'.join(current_folder_hashes) + '\\n')\n",
    "    print(len(current_folder_vectors))\n",
    "    if current_folder_vectors:\n",
    "        np_vector = np.array(current_folder_vectors)\n",
    "        mean_vector = np.mean(np_vector, axis=0)\n",
    "        mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
    "        final_vector_path = os.path.join(save_path, 'mean_vector.npz')\n",
    "        np.savez(final_vector_path, mean=mean_vector, all_vectors=np_vector)\n",
    "        print(f'Saved both mean and all vectors for {folder} to {final_vector_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5c0b0-2c56-443c-a3ad-6fff09cbf1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# for folder in folder_source:\n",
    "#     path = os.path.join(base_path, folder)\n",
    "#     save_path = os.path.join(clean_path, folder)\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#     vector = []\n",
    "#     for file in os.listdir(save_path):\n",
    "#         if file.endswith('.npy'):\n",
    "#             vec = np.load(os.path.join(save_path, file))\n",
    "#             vector.append(vec)\n",
    "    \n",
    "#     cos = cosine_similarity(vector, vector)\n",
    "#     print(len(cos))\n",
    "#     print(len(vector))\n",
    "#     sns.heatmap(cos[:10], annot=True,  fmt=\".2f\", cmap=\"coolwarm\")\n",
    "#     plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38120f-373e-4e01-b247-75f29a3dfeb7",
   "metadata": {},
   "source": [
    "# Collecting all the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d60ed-d832-4c49-af94-e5ac022598a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors, labels, filename = [], [], []\n",
    "clean_path = 'Clean_CV'\n",
    "print(folder_source)\n",
    "for folder in folder_source:\n",
    "    path = os.path.join(clean_path, folder)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.npy'):\n",
    "            vector = np.load(os.path.join(path, file))\n",
    "            all_vectors.append(vector)\n",
    "            labels.append(folder)\n",
    "            filename.append(file)\n",
    "all_vectors = np.array(all_vectors)\n",
    "all_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c187b-e8a4-4c03-b752-642b08036ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "# cluster on full embeddings\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')  \n",
    "cluster_labels = clusterer.fit_predict(all_vectors)\n",
    "\n",
    "print(\"Unique clusters:\", set(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d39abe-8870-4b7d-94e4-554af4ae8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "embedding_2d = reducer.fit_transform(all_vectors)  # shape -> (1030, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e08d1-9187-45a7-8d94-0fe2795e0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(embedding_2d[:,0], embedding_2d[:,1], c=cluster_labels, cmap='tab20', s=10)\n",
    "plt.title(\"HDBSCAN clusters of resumes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e1154-de40-4ca3-99b4-2d3d63432e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = list(set(labels))\n",
    "colors = [unique_labels.index(l) for l in labels]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(embedding_2d[:,0], embedding_2d[:,1], c=colors, cmap='tab20', s=10)\n",
    "plt.title(\"Resumes colored by folder\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93457907-6a55-4a78-95e1-b90a8b62b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folder_source:\n",
    "    indices = [i for i,l in enumerate(labels) if l==folder]\n",
    "    print(f\"{folder}: clusters = {set(cluster_labels[i] for i in indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c30954-a799-449b-9c30-f85b1983c5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
